---
title: "Regression and LASSO Analysis"
output: html_document
---

# Implementation of Regression and LASSO Analysis

## Set up parameters
```{r}
max_iter <- 800
lr <- 0.05
tol <- 1e-6
lambda <- 0.1
dir <- "/Users/khangnghiem/hcmus/courses/comp_stats/final/"
```

## David Gay Lasso Algorithm
```{r}
lasso_david_gay <- function(X, y, lambda = 0.1, max_iter = 1000, lr = 0.01, tol = 1e-6) {
    n <- nrow(X)
    p <- ncol(X)

    # Standardize X and center y
    X <- scale(X)
    y <- y - mean(y)

    beta_plus <- rep(0, p)
    beta_minus <- rep(0, p)

    # Adaptive learning rate
    lr_adjusted <- lr / max(colSums(X^2) / n) # Normalized by max eigenvalue

    # Initialize loss storage
    loss_history <- data.frame(
        iteration = 1:max_iter,
        least_squares_loss = rep(NA, max_iter),
        l1_penalty = rep(NA, max_iter),
        total_loss = rep(NA, max_iter)
    )

    for (iter in 1:max_iter) {
        beta <- beta_plus - beta_minus
        y_hat <- X %*% beta
        residual <- y_hat - y

        # Calculate and store loss components
        least_squares_loss <- sum(residual^2) / (2 * n)
        l1_penalty <- lambda * sum(abs(beta))
        total_loss <- least_squares_loss + l1_penalty

        loss_history$least_squares_loss[iter] <- least_squares_loss
        loss_history$l1_penalty[iter] <- l1_penalty
        loss_history$total_loss[iter] <- total_loss

        # Gradient of least squares loss
        grad <- t(X) %*% residual / n

        # Apply shrinkage more aggressively
        beta_plus_new <- beta_plus - lr_adjusted * (grad + lambda)
        beta_minus_new <- beta_minus - lr_adjusted * (-grad + lambda)

        # Soft-thresholding style projection
        beta_plus <- pmax(0, beta_plus_new)
        beta_minus <- pmax(0, beta_minus_new)

        # Forced zeroing when both components are small
        zero_idx <- (beta_plus < sqrt(tol)) & (beta_minus < sqrt(tol))
        beta_plus[zero_idx] <- 0
        beta_minus[zero_idx] <- 0

        # Convergence check
        beta_new <- beta_plus - beta_minus
        if (max(abs(beta_new - beta)) < tol) break
    }

    # Trim loss history to actual iterations run
    loss_history <- loss_history[1:iter, ]

    return(list(
        coefficients = beta_plus - beta_minus,
        loss_history = loss_history
    ))
}


run_lasso_analysis <- function(file_path, label_column, lambda = 0.1, exclude_columns = NULL, max_iter = 1000, lr = 0.01, tol = 1e-6) {
    data <- read.csv(file_path)

    # Exclude specified columns if any
    if (!is.null(exclude_columns) && exclude_columns != "") {
        data <- data[, !(names(data) %in% exclude_columns)]
    }

    # Prepare X and y
    X <- as.matrix(data[, -which(names(data) == label_column)])
    y <- data[[label_column]]

    # Run Lasso
    lasso_result <- lasso_david_gay(X, y, lambda = lambda, max_iter = max_iter, lr = lr, tol = tol)
    coefficients <- lasso_result$coefficients
    loss_history <- lasso_result$loss_history
    num_zero_coefficients <- sum(abs(coefficients) < 1e-3)
    print(paste("Number of coefficients close to 0.0:", num_zero_coefficients))

    # Plot convergence
    plot(loss_history$iteration, loss_history$total_loss,
        type = "l", xlab = "Iteration", ylab = "Loss",
        main = paste("Lasso Optimization Convergence for", basename(file_path)),
        ylim = c(0, max(loss_history$total_loss, na.rm = TRUE))
    )
    lines(loss_history$iteration, loss_history$least_squares_loss, col = "blue")
    lines(loss_history$iteration, loss_history$l1_penalty, col = "red")
    legend("topright",
        legend = c("Total Loss", "Least Squares", "L1 Penalty"),
        col = c("black", "blue", "red"), lty = 1
    )

    return(list(coefficients = coefficients, loss_history = loss_history))
}
```

## Linear Regression Algorithm

```{r}
linear_regression_ols <- function(X, y, max_iter = 1000, lr = 0.01, tol = 1e-6) {
    n <- nrow(X)
    p <- ncol(X)

    # Standardize X and center y (important for gradient descent)
    X <- scale(X)
    y <- y - mean(y)

    # Initialize coefficients
    beta <- rep(0, p)

    # Initialize loss storage
    loss_history <- data.frame(
        iteration = 1:max_iter,
        mse_loss = rep(NA, max_iter)
    )

    for (iter in 1:max_iter) {
        # Calculate predictions and residuals
        y_hat <- X %*% beta
        residual <- y_hat - y

        # Calculate and store MSE loss (1/2n for gradient descent scaling)
        mse_loss <- sum(residual^2) / (2 * n)
        loss_history$mse_loss[iter] <- mse_loss

        # Calculate gradient
        grad <- t(X) %*% residual / n

        # Update coefficients
        beta <- beta - lr * grad

        # Convergence check (stop if change is small)
        if (iter > 1 && abs(loss_history$mse_loss[iter - 1] - mse_loss) < tol) {
            loss_history <- loss_history[1:iter, ]
            break
        }
    }

    return(list(
        coefficients = beta,
        loss_history = loss_history
    ))
}

run_linear_regression_analysis <- function(file_path, label_column, exclude_columns = NULL, max_iter = 1000, lr = 0.01, tol = 1e-6) {
    data <- read.csv(file_path)

    # Exclude specified columns if any
    if (!is.null(exclude_columns) && exclude_columns != "") {
        data <- data[, !(names(data) %in% exclude_columns)]
    }

    # Prepare X and y
    X <- as.matrix(data[, -which(names(data) == label_column)])
    y <- data[[label_column]]

    # Run linear regression
    regression_result <- linear_regression_ols(X, y, max_iter = max_iter, lr = lr, tol = tol)
    coefficients <- regression_result$coefficients
    loss_history <- regression_result$loss_history

    # Plot convergence
    plot(loss_history$iteration, loss_history$mse_loss,
        type = "l", xlab = "Iteration", ylab = "Loss",
        main = paste("Linear Regression Convergence for", basename(file_path))
    )
    grid()

    return(list(coefficients = coefficients, loss_history = loss_history))
}
```

## Run Analysis
```{r}
run_linear_regression_analysis(paste0(dir, "prostate.csv"), label_column = "lpsa", exclude_columns = "train", max_iter = max_iter, lr = lr, tol = tol)
run_lasso_analysis(paste0(dir, "prostate.csv"), label_column = "lpsa", lambda = 0.1, exclude_columns = "train", max_iter = max_iter, lr = lr, tol = tol)

run_linear_regression_analysis(paste0(dir, "car_price.csv"), label_column = "price_thousands_USD", exclude_columns = NULL, max_iter = max_iter, lr = lr, tol = tol)
run_lasso_analysis(paste0(dir, "car_price.csv"), label_column = "price_thousands_USD", lambda = 0.1, exclude_columns = NULL, max_iter = max_iter, lr = lr, tol = tol)
```

# Results

For prostate data, 3 coefficients out of 8 were reduced to 0.0 from Lasso regression
For car_price data, 130 coefficients out of 201 were reduced to 0.0 from Lasso regression

# Summary

Lasso regression leads to a sparse solution, which is useful for feature selection. The algorithm converges quickly and effectively reduces the coefficients of less important features to zero, making it easier to interpret the model. The adaptive learning rate helps in speeding up convergence, especially when dealing with large datasets.
The loss from Lasso regression is however, higher than that of OLS regression, which is expected as Lasso adds a penalty term to the loss function. The trade-off between bias and variance is evident in the results, where Lasso may introduce some bias but reduces variance significantly by eliminating irrelevant features.
The linear regression algorithm converges to a solution that minimizes the mean squared error, but it does not perform feature selection. The coefficients obtained from OLS regression are generally larger than those from Lasso regression, as OLS does not penalize the coefficients.

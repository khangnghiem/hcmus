# Problem 3: Oil Spills Analysis

## Setup Code
```{r}
year <- c(
  1974, 1975, 1976, 1977, 1978, 1979, 1980,
  1981, 1982, 1983, 1984, 1985, 1986, 1987,
  1988, 1989, 1990, 1991, 1992, 1993, 1994,
  1995, 1996, 1997, 1998, 1999
)
N <- spills <- c(
  2, 5, 3, 3, 1, 5, 2, 2, 1, 1, 1, 2, 3,
  4, 2, 2, 3, 2, 1, 0, 0, 1, 0, 0, 0, 0
)
b1 <- importexport <- c(
  0.720, 0.850, 1.120, 1.345, 1.290, 1.260, 1.015, 0.870, 0.750, 0.605, 0.570,
  0.540, 0.720, 0.790, 0.840, 0.995, 1.030, 0.975, 1.070, 1.190, 1.290, 1.235,
  1.340, 1.440, 1.450, 1.510
)
b2 <- domestic <- c(
  0.22, 0.17, 0.15, 0.20, 0.59, 0.64, 0.84, 0.87, 0.94, 0.99, 0.92,
  1.00, 0.99, 1.06, 1.00, 0.88, 0.82, 0.82, 0.76, 0.66, 0.65, 0.59,
  0.56, 0.51, 0.42, 0.44
)
alpha0 <- c(1.0, 1.0)

# Define necessary functions
log_likelihood <- function(alpha) {
  lambda <- alpha[1] * b1 + alpha[2] * b2
  if (any(lambda <= 0)) {
    return(-Inf)
  }
  sum(N * log(lambda) - lambda)
}

gradient <- function(alpha) {
  lambda <- alpha[1] * b1 + alpha[2] * b2
  c(sum((N / lambda - 1) * b1), sum((N / lambda - 1) * b2))
}

hessian <- function(alpha) {
  lambda <- alpha[1] * b1 + alpha[2] * b2
  if (any(lambda <= 0)) {
    return(matrix(NA, 2, 2))
  }
  H11 <- -sum(N * b1^2 / lambda^2)
  H12 <- -sum(N * b1 * b2 / lambda^2)
  H22 <- -sum(N * b2^2 / lambda^2)
  matrix(c(H11, H12, H12, H22), 2, 2)
}

fisher_info <- function(alpha) {
  lambda <- alpha[1] * b1 + alpha[2] * b2
  if (any(lambda <= 0)) {
    return(matrix(NA, 2, 2))
  }
  I11 <- sum(b1^2 / lambda)
  I21 <- I12 <- sum(b1 * b2 / lambda)
  I22 <- sum(b2^2 / lambda)
  matrix(c(I11, I12, I21, I22), 2, 2)
}
```
## (a) Log-Likelihood Function Derivation

### Given Model Specification
- $N_i \sim \text{Poisson}(\lambda_i)$
- $\lambda_i = \alpha_1 b_{i1} + \alpha_2 b_{i2}$

### Likelihood Function
The likelihood function for observations ${N_i}$ is:
$$
L(\alpha_1, \alpha_2) = \prod_{i=1}^{n} \frac{e^{-\lambda_i} \lambda_i^{N_i}}{N_i!}
$$

### Log-Likelihood Function
$$\begin{aligned}
\ell(\alpha_1, \alpha_2) &= \log(\prod_{i=1}^{n} \frac{e^{-\lambda_i} \lambda_i^{N_i}}{N_i!}) \\
\ell(\alpha_1, \alpha_2) &= \sum_{i=1}^{n}(\log \frac{e^{-\lambda_i} \lambda_i^{N_i}}{N_i!}) \\
\ell(\alpha_1, \alpha_2) &= \sum_{i=1}^n \left[ N_i \log(\lambda_i) - \lambda_i - \log(N_i!) \right] \\
\ell(\alpha_1, \alpha_2) &= \sum_{i=1}^n \left[ N_i \log(\alpha_1 b_{i1} + \alpha_2 b_{i2}) - (\alpha_1 b_{i1} + \alpha_2 b_{i2}) - \log(N_i!) \right] \\
\end{aligned}$$

---

## (b) Newton-Raphson Algorithm

### First Derivatives (Gradient)
$$
\begin{aligned}
\frac{\partial \ell}{\partial \alpha_1} &= \frac{\partial \sum_{i=1}^n \left[ N_i \log(\alpha_1 b_{i1} + \alpha_2 b_{i2}) - (\alpha_1 b_{i1} + \alpha_2 b_{i2}) - \log(N_i!) \right] }{\partial \alpha_1}\\
\frac{\partial \ell}{\partial \alpha_1} &= \sum_{i=1}^n \frac{\partial}{\partial \alpha_1}N_i \log(\alpha_1 b_{i1} + \alpha_2 b_{i2}) - \sum_{i=1}^n \frac{\partial}{\partial \alpha_1}(\alpha_1 b_{i1} + \alpha_2 b_{i2}) - \sum_{i=1}^n \frac{\partial \log(N_i!) }{\partial \alpha_1}\\
\frac{\partial \ell}{\partial \alpha_1} &= \sum_{i=1}^n \frac{N_i}{(\alpha_1 b_{i1} + \alpha_2 b_{i2})} \frac{\partial}{\partial \alpha_1}(\alpha_1 b_{i1} + \alpha_2 b_{i2}) - \sum_{i=1}^n \frac{\partial}{\partial \alpha_1}(\alpha_1 b_{i1} + \alpha_2 b_{i2})\\
\frac{\partial \ell}{\partial \alpha_1} &= \sum_{i=1}^n \frac{N_i}{(\alpha_1 b_{i1} + \alpha_2 b_{i2})} b_{i1} - \sum_{i=1}^n b_{i1}\\
\frac{\partial \ell}{\partial \alpha_1} &= \sum_{i=1}^n \left[ \frac{N_i}{(\alpha_1 b_{i1} + \alpha_2 b_{i2})}b_{i1} - b_{i1} \right] \\
\frac{\partial \ell}{\partial \alpha_1} &= \sum_{i=1}^n \left[ \frac{N_i b_{i1} }{(\alpha_1 b_{i1} + \alpha_2 b_{i2})} - b_{i1} \right] \\
\frac{\partial \ell}{\partial \alpha_1} &= \sum_{i=1}^n \left[ \frac{N_i b_{i1} }{\lambda_i} - b_{i1} \right] \\
Likewise: \\
\frac{\partial \ell}{\partial \alpha_2} &= \sum_{i=1}^n \left[ \frac{N_i b_{i2}}{\lambda_i} - b_{i2} \right] \\
\end{aligned}
$$

### Second Derivatives (Hessian)
$$
\begin{aligned}
\frac{\partial^2 \ell}{\partial \alpha_1^2} &= \frac{\partial}{\partial \alpha_1} \sum_{i=1}^n \left[ \frac{N_i b_{i1} }{(\alpha_1 b_{i1} + \alpha_2 b_{i2})} - b_{i1} \right] \\
\frac{\partial^2 \ell}{\partial \alpha_1^2} &= \sum_{i=1}^n \frac{\partial}{\partial \alpha_1} \left[ \frac{N_i b_{i1} }{(\alpha_1 b_{i1} + \alpha_2 b_{i2})} \right] \\
\frac{\partial^2 \ell}{\partial \alpha_1^2} &= -\sum_{i=1}^n \frac{N_i b_{i1}}{(\alpha_1 b_{i1} + \alpha_2 b_{i2})^2} \frac{\partial}{\partial \alpha_1} \left[ (\alpha_1 b_{i1} + \alpha_2 b_{i2}) \right] \\
\frac{\partial^2 \ell}{\partial \alpha_1^2} &= -\sum_{i=1}^n \frac{N_i (b_{i1})^2}{\lambda_i^2} \\
Likewise: \\
\frac{\partial^2 \ell}{\partial \alpha_1 \partial \alpha_2} &= \sum_{i=1}^n \frac{\partial}{\partial \alpha_2} \left[ \frac{N_i b_{i1} }{(\alpha_1 b_{i1} + \alpha_2 b_{i2})} \right] \\
\frac{\partial^2 \ell}{\partial \alpha_1 \partial \alpha_2} &= -\sum_{i=1}^n \frac{N_i b_{i1}}{(\alpha_1 b_{i1} + \alpha_2 b_{i2})^2} \frac{\partial}{\partial \alpha_2} \left[ (\alpha_1 b_{i1} + \alpha_2 b_{i2}) \right] \\
\frac{\partial^2 \ell}{\partial \alpha_1 \partial \alpha_2} &= -\sum_{i=1}^n \frac{N_i b_{i1}b_{i2}}{\lambda_i^2} \\
And: \\
\frac{\partial^2 \ell}{\partial \alpha_2 \partial \alpha_1} &= -\sum_{i=1}^n \frac{N_i b_{i1}b_{i2}}{\lambda_i^2} \\
\frac{\partial^2 \ell}{\partial \alpha_2^2} &= -\sum_{i=1}^n \frac{N_i (b_{i2})^2}{\lambda_i^2}
\end{aligned}
$$

### Newton-Raphson Formula

The Newton-Raphson update formula for parameter estimation is given by:

$$\begin{aligned}
\alpha^{(k+1)} &= \alpha^{(k)} - [H^{(k)}]^{-1} g^{(k)} \\
&= \alpha^{(k)} - \begin{bmatrix}
\frac{\partial^2 \ell}{\partial \alpha_1^2} & \frac{\partial^2 \ell}{\partial \alpha_1 \partial \alpha_2} \\
\frac{\partial^2 \ell}{\partial \alpha_2 \partial \alpha_1} & \frac{\partial^2 \ell}{\partial \alpha_2^2}
\end{bmatrix} \begin{bmatrix}
\frac{\partial \ell}{\partial \alpha_1} \\
\frac{\partial \ell}{\partial \alpha_2}
\end{bmatrix} \\
&= \alpha^{(k)} - \begin{bmatrix}
-\sum_{i=1}^n \frac{N_i (b_{i1})^2}{\lambda_i^2} & -\sum_{i=1}^n \frac{N_i b_{i1}b_{i2}}{\lambda_i^2} \\
-\sum_{i=1}^n \frac{N_i b_{i1}b_{i2}}{\lambda_i^2} & -\sum_{i=1}^n \frac{N_i (b_{i2})^2}{\lambda_i^2}
\end{bmatrix}^{-1}_{(k)} \begin{bmatrix}
\sum_{i=1}^n \left[ \frac{N_i b_{i1}}{\lambda_i} - b_{i1} \right] \\
\sum_{i=1}^n \left[ \frac{N_i b_{i2}}{\lambda_i} - b_{i2} \right] \\
\end{bmatrix}^{(k)} \\
\end{aligned}$$



### Algorithm Steps
1. Initialize $\alpha^{(0)} = [0.1, 0.1]^T$
2. For iteration $k=1,2,...$:
   - Compute $\lambda_i^{(k)} = \alpha_1^{(k)}b_{i1} + \alpha_2^{(k)}b_{i2}$
   - Calculate gradient vector $g^{(k)}$
   - Calculate Hessian matrix $H^{(k)}$
   - Update: $\alpha^{(k+1)} = \alpha^{(k)} - [H^{(k)}]^{-1}g^{(k)}$
3. Stop when $||\alpha^{(k+1)} - \alpha^{(k)}|| < \epsilon$

### Implementation Code
```{r}
newton_raphson <- function(alpha0, tol = 1e-8, max_iter = 1000) {
  path <- matrix(alpha0, ncol = 2)
  for (i in 1:max_iter) {
    g <- gradient(path[i, ])
    H <- hessian(path[i, ])
    delta <- solve(H, -g)
    new_point <- path[i, ] + delta
    path <- rbind(path, new_point)
    cat("Iteration:", i, "| alpha: (", new_point, ")\n")
    if (max(abs(delta)) < tol) break
  }
  return(path)
}

nr_path <- newton_raphson(alpha0)
print(nr_path)
```

## (c) Fisher Scoring Algorithm

### Fisher Information Matrix

$$
\begin{aligned}
I(\alpha_1, \alpha_2) &= -E \left[ \frac{\partial^2 \ell}{\partial \alpha_i \partial \alpha_j} \right] \\
\end{aligned}
$$

For the elements of the Fisher Information Matrix, we have:

$$
\begin{aligned}
I_{11} &= E \left[ \sum_{i=1}^n \frac{N_i (b_{i1})^2}{\lambda_i^2} \right] \\
I_{11} &= \sum_{i=1}^n \frac{(b_{i1})^2}{\lambda_i} (\text{assuming } N_i = \lambda_i)\\
Likewise: \\
I_{12} &= E \left[ \sum_{i=1}^n \frac{N_i b_{i1} b_{i2}}{\lambda_i^2} \right] \\
I_{12} &= \sum_{i=1}^n \frac{b_{i1} b_{i2}}{\lambda_i} \\
I_{21} &= I_{12} \\
I_{22} &= E \left[ \sum_{i=1}^n \frac{N_i (b_{i2})^2}{\lambda_i^2} \right] \\
I_{22} &= \sum_{i=1}^n \frac{(b_{i2})^2}{\lambda_i} \\
\end{aligned} \\
Finally:
I(\alpha_1, \alpha_2) = \begin{bmatrix}
\sum_{i=1}^n \frac{(b_{i1})^2}{\lambda_i} & \sum_{i=1}^n \frac{b_{i1} b_{i2}}{\lambda_i} \\
\sum_{i=1}^n \frac{b_{i1} b_{i2}}{\lambda_i} & \sum_{i=1}^n \frac{(b_{i2})^2}{\lambda_i}
\end{bmatrix}
$$

### Fisher Scoring

The Fisher Scoring update formula for parameter estimation is given by:
$$
\begin{aligned}
\alpha^{(k+1)} &= \alpha^{(k)} + [I(\alpha^{(k)})]^{-1} g^{(k)} \\
&= \alpha^{(k)} + \begin{bmatrix}
\sum_{i=1}^n \frac{(b_{i1})^2}{\lambda_i} & \sum_{i=1}^n \frac{b_{i1} b_{i2}}{\lambda_i} \\
\sum_{i=1}^n \frac{b_{i1} b_{i2}}{\lambda_i} & \sum_{i=1}^n \frac{(b_{i2})^2}{\lambda_i}
\end{bmatrix}^{-1} \begin{bmatrix}
\sum_{i=1}^n \left[ \frac{N_i b_{i1}}{\lambda_i} - b_{i1} \right] \\
\sum_{i=1}^n \left[ \frac{N_i b_{i2}}{\lambda_i} - b_{i2} \right] \\
\end{bmatrix} \\
\end{aligned}
$$


### Implementation Code
```{r}
fisher_scoring <- function(alpha0, tol = 1e-6, max_iter = 100) {
  path <- matrix(alpha0, ncol = 2)
  for (i in 1:max_iter) {
    g <- gradient(path[i, ])
    I <- fisher_info(path[i, ])
    if (any(is.na(c(g, I)))) break
    delta <- solve(I, g)
    new_point <- path[i, ] + delta
    path <- rbind(path, new_point)
    cat("Iteration:", i, "| alpha: (", new_point, ")\n")
    if (max(abs(delta)) < tol) break
  }
  return(path)
}

fs_path <- fisher_scoring(alpha0)
print(fs_path)
```

### Comparing Results of Newton-Raphson and Fisher Scoring

- Newton-Raphson is computationally faster due to the exact Hessian matrix calculation, while Fisher Scoring approximates the Hessian using the Fisher Information matrix.
- Newton-Raphson results converge to the MLE estimates of $\alpha_1$ and $\alpha_2$ after 4 iterations, while Fisher Scoring converges after 12 iterations. 
- Both algorithms provide similar estimates for the parameters.

## (d) Standard Errors of MLE Estimates

$$\begin{aligned}
\hat{\lambda}_i = \hat{\alpha}_1 b_{i1} + \hat{\alpha}_2 b_{i2} \\
\text{SE}(\hat{\alpha}_j) = \sqrt{\text{diag} (I^{-1}(\hat{\alpha}))_{j}} \\
\text{SE}(\hat{\alpha}_1) = \sqrt{\text{diag} (I^{-1}(\hat{\alpha}))_{1}} \\
\text{SE}(\hat{\alpha}_1) = \sqrt{\left[ I(\hat{\alpha})^{-1} \right]_{11}} \\
\text{SE}(\hat{\alpha}_2) = \sqrt{\left[ I(\hat{\alpha})^{-1} \right]_{22}} \\
\end{aligned}$$

### Implementation Code

Newton-Raphson method has lower standard errors compared to Fisher Scoring method.

```{r}
alpha_newton <- as.vector(tail(newton_raphson(alpha0), 1))
se_newton <- sqrt(diag(solve(-hessian(alpha_newton))))
alpha_fisher <- as.vector(tail(fisher_scoring(alpha0), 1))
se_fisher <- sqrt(diag(solve(fisher_info(alpha_fisher))))

cat("MLE Estimates (Newton):", round(alpha_newton, 5), "\n")
cat("MLE Estimates (Fisher):", round(as.numeric(alpha_fisher), 5), "\n")

cat("Standard Errors (Newton):", round(se_newton, 5), "\n")
cat("Standard Errors (Fisher):", round(se_fisher, 5), "\n")
```

## (e) Quasi-Newton Method with Two Initial Matrix Choices

### Algorithm Steps

1. Approximates the inverse Hessian using gradient updates.
2. Update rule: $\alpha^{(k+1)} = \alpha^{(k)} - p_k M^{(k)} g^{(k)}$ where p_k is the step size.
3. BFGS update for $M^{(k)}$:
4. $M^{(k+1)} = M^{(k)} + \frac{y^{(k)}(y^{(k)})^T}{(y^{(k)})^T s^{(k)}} - \frac{M^{(k)} s^{(k)}(M^{(k)} s^{(k)})^T}{s^{(k)T} M^{(k)} s^{(k)}}$
5. Initialize $\alpha^{(0)} = [0.1, 0.1]^T$
6. Choose an initial approximation of the Hessian matrix $B^{(0)}$
7. For iteration $k=1,2,...$:
  - Compute $\lambda_i^{(k)} = \alpha_1^{(k)}b_{i1} + \alpha_2^{(k)}b_{i2}$
  - Calculate gradient vector $g^{(k)}$
  - Update: $\alpha^{(k+1)} = \alpha^{(k)} - [B^{(k)}]^{-1} g^{(k)}$
  - Update $B^{(k+1)}$ using the BFGS update formula
  - Stop when $||\alpha^{(k+1)} - \alpha^{(k)}|| < \epsilon$
8. Choose another initial approximation of the Hessian matrix $B^{(0)}$ and repeat the process


### Implementation Code


```{r}
quasi_newton <- function(alpha0, M0, tol = 1e-6, max_iter = 100) {
  path <- matrix(alpha0, nrow = 1)
  M <- M0
  g_current <- gradient(alpha0)
  for (i in 1:max_iter) {
    step <- as.vector(-M %*% g_current)
    new_alpha <- path[i, ] + step
    # Ensure alpha > 0 to keep lambda valid
    if (any(new_alpha <= 0)) {
      warning("Negative alpha values: ", new_alpha, ". Step rejected.")
      break
    }
    s <- as.vector(new_alpha - path[i, ])
    g_new <- gradient(new_alpha)
    y <- as.vector(g_new - g_current)

    # Compute rho and update M
    sy <- sum(y * s)
    if (sy == 0) break # Avoid division by zero
    rho <- 1 / sy

    # Update M using outer products
    I <- diag(2)
    term1 <- I - rho * outer(s, y)
    term2 <- I - rho * outer(y, s)
    M <- term1 %*% M %*% term2 + rho * outer(s, s)

    path <- rbind(path, new_alpha)
    g_current <- g_new
    if (max(abs(step)) < tol) break
  }
  path
}

qn_negI_path <- quasi_newton(alpha0, M0 = -diag(2))
qn_negFisher_path <- quasi_newton(alpha0, M0 = -solve(fisher_info(alpha0)))
print(qn_negI_path)
print(qn_negFisher_path)
```

### Comparison
Method 1 (Default BFGS): 
- Starts with identity matrix. 
- Converges reliably but may require more iterations.
- Very often runs into negative alpha values, which are invalid for the Poisson model. 
Method 2 (Fisher-scaled BFGS): 
- Uses diagonal Fisher Information for parameter scaling. 
- Converges faster due to better initial step sizing. 
- Final estimates are identical.
- Typically uses fewer iterations.
- Converges faster.
- Avoids negative values.

## (f) Plot Optimization Paths

- The contour plot shows the log-likelihood surface with darker regions indicating higher values.
- Newton-Raphson (red) and Fisher Scoring (blue) show rapid convergence due to exact second-order information.
- Quasi-Newton with -I (green) exhibits more iterations due to initial approximation mismatch.
- Quasi-Newton with -Fisher (purple) converges faster as it starts with better curvature information.

```{r}
# Generate optimization paths
library(ggplot2)
set.seed(123)

print(nr_path)
print(fs_path)
print(qn_negI_path)
print(qn_negFisher_path)

# Generate grid coordinates
alpha1_grid <- seq(0.8, 1.8, length.out = 200)
alpha2_grid <- seq(0.7, 1.7, length.out = 200)

# Calculate log-likelihood surface
logL_grid <- outer(
  alpha1_grid, alpha2_grid,
  Vectorize(function(a1, a2) log_likelihood(c(a1, a2)))
)

# Create contour data using base R
contour_data <- expand.grid(alpha1 = alpha1_grid, alpha2 = alpha2_grid)
contour_data$logL <- as.vector(logL_grid)

# Convert path matrices to data frames
nr_df <- data.frame(nr_path, Method = "Newton-Raphson")
fs_df <- data.frame(fs_path, Method = "Fisher-Scoring")
qn_negI_df <- data.frame(qn_negI_path, Method = "Quasi-Newton (-I)")
qn_negFisher_df <- data.frame(qn_negFisher_path, Method = "Quasi-Newton (-Fisher)")

# Combine all paths
all_paths <- rbind(nr_df, fs_df, qn_negI_df, qn_negFisher_df)
colnames(all_paths) <- c("alpha1", "alpha2", "Method")

# Create plot
ggplot(contour_data, aes(x = alpha1, y = alpha2)) +
  geom_contour(aes(z = logL), bins = 30, color = "gray70") +
  geom_path(
    data = all_paths, aes(color = Method, group = Method),
    linewidth = 0.8, alpha = 0.8
  ) +
  geom_point(data = all_paths, aes(color = Method), size = 1.5) +
  geom_point(aes(x = 1.097153, y = 0.9375547),
    color = "red", size = 4, shape = 18
  ) +
  scale_color_manual(values = c(
    "Newton-Raphson" = "#1f77b4",
    "Fisher-Scoring" = "#ff7f0e",
    "Quasi-Newton (-I)" = "#2ca02c",
    "Quasi-Newton (-Fisher)" = "#d62728"
  )) +
  labs(
    title = "Optimization Paths Comparison",
    x = expression(alpha[1]),
    y = expression(alpha[2]),
    subtitle = "Red diamond shows true MLE values"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.grid = element_blank(),
    panel.border = element_rect(fill = NA, color = "gray80")
  )
```
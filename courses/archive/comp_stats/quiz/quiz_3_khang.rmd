```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Bài tập 1.2

Hàm mật độ xác suất của phân phối Cauchy:

$$ f(x) = \frac{1}{\theta\pi (1 + ((x - \alpha)/\theta)^2)} $$


### Hàm mật độ của phân phối Cauchy
```{r}
cauchy_cdf <- function(t, alpha, theta, method = "simpson", n = 1000) {
    a_scaled <- (t - alpha) / theta
    integrand <- function(y) 1 / (1 + y^2)

    if (a_scaled == 0) {
        return(0.5)
    }

    if (a_scaled > 0) {
        integral <- switch(method,
            "riemann" = Riemann(integrand, 0, a_scaled, n),
            "trapezoidal" = Trapezoidal(integrand, 0, a_scaled, n),
            "simpson" = Simpson(integrand, 0, a_scaled, n)
        )
    } else {
        integral <- -switch(method,
            "riemann" = Riemann(integrand, a_scaled, 0, n),
            "trapezoidal" = Trapezoidal(integrand, a_scaled, 0, n),
            "simpson" = Simpson(integrand, a_scaled, 0, n)
        )
    }

    0.5 + integral / pi
}
```

### Phương pháp Riemann
```{r}
Riemann <- function(f, a, b, n) {
    h <- (b - a) / n
    x <- seq(a, b - h, length.out = n)
    sum(f(x) * h)
}
```

### Phương pháp Trapezoidal
```{r}
Trapezoidal <- function(f, a, b, n) {
    h <- (b - a) / n
    x <- seq(a, b, length.out = n + 1)
    sum(f(x[1:n]) + f(x[2:(n + 1)])) * h / 2
}
```

### Phương pháp Simpson
```{r}
Simpson <- function(f, a, b, n) {
    h <- (b - a) / n
    x <- seq(a, b, length.out = n + 1)
    sum_result <- 0
    for (i in 1:n) {
        mid <- (x[i] + x[i + 1]) / 2
        sum_result <- sum_result + (f(x[i]) + 4 * f(mid) + f(x[i + 1])) * h / 6
    }
    sum_result
}
```

### So sánh kết quả
```{r}
test_cauchy_cdf <- function(t_values, alpha, theta, n = 1000) {
    for (t in t_values) {
        approx_riemann <- cauchy_cdf(t, alpha, theta, "riemann", n)
        approx_trapezoidal <- cauchy_cdf(t, alpha, theta, "trapezoidal", n)
        approx_simpson <- cauchy_cdf(t, alpha, theta, "simpson", n)
        exact <- pcauchy(t, location = alpha, scale = theta)

        cat(sprintf("\nFor t = %.1f, alpha = %.1f, theta = %.1f:\n", t, alpha, theta))
        cat(sprintf("Riemann:    %.6f\n", approx_riemann))
        cat(sprintf("Trapezoidal:%.6f\n", approx_trapezoidal))
        cat(sprintf("Simpson:    %.6f\n", approx_simpson))
        cat(sprintf("Exact:      %.6f\n", exact))
    }
}
test_cauchy_cdf(t_values = c(-1, 0, 1, 2), alpha = 0, theta = 1)
```

### Kết luận

**Phương pháp Riemann xấp xỉ kém nhất** 


# Bài tập 2.3.2

## (a) Phương pháp Monte Carlo Importance Sampling

**Ý tưởng**: Sử dụng phân phối đề xuất \( q(x) = \mathcal{N}(0, 1) \). Tính trọng số \( w(x) = \frac{h(x)}{q(x)} \) và ước lượng \(\sigma^2\) bằng trung bình trọng số của \( x^2 \).

```{r part_a}
importance_sampling <- function(n, proposal_sd = 1) {
    # Tạo mẫu từ phân phối đề xuất
    x <- rnorm(n, 0, proposal_sd)

    # Tính trọng số importance sampling
    w <- exp(-abs(x)^3 / 3) / dnorm(x, 0, proposal_sd)

    # Ước lượng sigma^2
    sigma2_est <- sum(x^2 * w) / sum(w)
    return(sigma2_est)
}
```

---

## (b) Phương pháp Tổng Riemann

**Ý tưởng**: 
1. Sử dụng MCMC (Metropolis-Hastings) để lấy mẫu từ phân phối \( p(x) \propto \exp(-|x|^3/3) \)
2. Sắp xếp mẫu và tính toán ước lượng dựa trên công thức Riemann.

```{r part_b}
riemann_estimator <- function(n_samples_mcmc, step_size = 1, burn_in = 1000) {
    # Khởi tạo MCMC
    samples <- numeric(n_samples_mcmc + burn_in)
    current <- 0

    # Thuật toán Metropolis-Hastings
    for (i in 1:(n_samples_mcmc + burn_in)) {
        y <- rnorm(1, current, step_size)
        log_alpha <- (-abs(y)^3 / 3) - (-abs(current)^3 / 3)
        if (log(runif(1)) < log_alpha) current <- y
        samples[i] <- current
    }

    # Loại bỏ burn-in và sắp xếp mẫu
    samples <- samples[(burn_in + 1):(n_samples_mcmc + burn_in)]
    sorted_samples <- sort(samples)

    # Tính tổng Riemann
    n <- length(sorted_samples)
    sum_num <- sum_den <- 0
    for (i in 1:(n - 1)) {
        delta <- sorted_samples[i + 1] - sorted_samples[i]
        h_val <- exp(-abs(sorted_samples[i])^3 / 3)
        sum_num <- sum_num + sorted_samples[i]^2 * delta * h_val
        sum_den <- sum_den + delta * h_val
    }
    return(sum_num / sum_den)
}
```

---

## (c) Mô phỏng so sánh

**Thiết lập**:
- Giá trị thực: \(\sigma^2 = \frac{3^{1/3}}{\Gamma(1/3)} \approx 0.7762\)
- Số lần mô phỏng: 100
- Cỡ mẫu: \( n = 10^4 \) cho mỗi phương pháp

```{r part_c}
# Giá trị thực
sigma2_true <- 3^(1 / 3) / gamma(1 / 3)

# Mô phỏng
n_sim <- 100
results <- data.frame(
    method_a = replicate(n_sim, importance_sampling(1e4)),
    method_b = replicate(n_sim, riemann_estimator(1e4))
)

# Tính các thống kê
compute_stats <- function(x) {
    c(
        Bias = mean(x) - sigma2_true,
        Variance = var(x),
        MSE = mean((x - sigma2_true)^2)
    )
}

stats_a <- compute_stats(results$method_a)
stats_b <- compute_stats(results$method_b)

# Hiển thị kết quả
df_results <- data.frame(
    Phương_pháp = c("(a) Importance Sampling", "(b) Riemann"),
    Bias = c(stats_a["Bias"], stats_b["Bias"]),
    Phương_sai = c(stats_a["Variance"], stats_b["Variance"]),
    MSE = c(stats_a["MSE"], stats_b["MSE"])
)

# Định dạng số thập phân và in bảng
print(format(df_results, digits = 5, nsmall = 5), row.names = FALSE)
```

### Kết luận

1. **Phương pháp (a)** (Monte Carlo Importance Sampling):
   - Hiệu quả khi phân phối đề xuất được chọn tốt
   - Đòi hỏi cỡ mẫu lớn để ổn định

2. **Phương pháp (b)** (Tổng Riemann):
   - Có độ chệch do xấp xỉ tích phân
   - Đòi hỏi số chiều thấp để ổn định

Kết quả mô phỏng phù hợp với lý thuyết: Importance Sampling cho MSE thấp hơn trong hầu hết trường hợp.
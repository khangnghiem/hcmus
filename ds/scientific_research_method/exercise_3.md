# Level 1 - List all paper summaries

Problem to solve: Use of Machine Learning in Robots

| Relevance to Problem | Title                                                                                                                        | Link                                                                                                               | Comment                                                                                             | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| -------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| More Specific        | A Reinforcement Learning Environment for Automatic Code Optimization in The MLIR Compiler                                    | <https://arxiv.org/pdf/2409.11068>                                                                                 | Useful in improving current RL model                                                                | This paper presents a reinforcement learning (RL) environment for automated code optimization in MLIR, enhancing machine learning performance. Key contributions: (1) the first RL environment for MLIR, (2) a hierarchical action space enabling efficient optimizations, and (3) an RL agent matching/exceeding frameworks like TensorFlow in benchmarks. Experiments demonstrate accelerated MLIR optimizations with reduced computational overhead. While highlighting RL's potential in compiler optimization, future work includes expanding transformation techniques and developing cost models to accelerate training.                                                                                                                                                                                                                                              |
| More Specific        | RL-GPT: Integrating Reinforcement Learning and Code-as-policy                                                                | <https://openreview.net/forum?id=LEzx6QRkRH>                                                                       | Useful AI in video games                                                                            | This research paper introduces RL-GPT, a new system that combines Large Language Models (LLMs) and Reinforcement Learning (RL) to help computer agents learn tasks in video games, like Minecraft. It uses a two-part approach where one part analyzes what actions to take and the other part handles the coding and execution of those actions. This method is more efficient and effective than traditional methods, allowing the agent to achieve complex tasks, such as finding diamonds, much faster and with less human help.                                                                                                                                                                                                                                                                                                                                         |
| More General         | FALCON: Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization                                       | <https://arxiv.org/pdf/2410.21349>                                                                                 | Introduce long-term and short-term feedback                                                         | FALCON is a reinforcement learning-based framework that improves code generation by using long-term and short-term memory feedback. It integrates diverse feedback types to enhance code quality and adaptability. FALCON outperforms other models in benchmarks like APPS and LeetCode, with plans for broader language and task support in the future.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| More General         | Process-Supervised Reinforcement Learning for Code Generation                                                                | <https://arxiv.org/pdf/2502.01715>                                                                                 | Use Process Supervision Instead of Outcome Supervision                                              | PRLCoder enhances code generation in LLMs using process-supervised reinforcement learning (RL), providing step-by-step feedback for improved training stability and accuracy. It automates data generation, trains a reward model for fine-grained feedback, and integrates with PPO for RL training. PRLCoder outperforms outcome-supervised methods by 4.4%–10.5% on benchmarks, converging faster and more reliably. While limited in dataset diversity and scalability, it reduces manual labeling costs and holds potential for broader reasoning tasks.                                                                                                                                                                                                                                                                                                                |
| More General         | Model-based RL with Optimistic Posterior Sampling: Structural Conditions and Sample Complexity                               | <https://proceedings.neurips.cc/paper_files/paper/2022/file/e536e43b01a4387a2282c2b04103c802-Paper-Conference.pdf> | Balance exploration and exploitation                                                                | The paper explores model-based reinforcement learning (MBRL) with optimistic posterior sampling, analyzing its structural conditions and sample complexity. It establishes theoretical guarantees for efficient exploration and learning, demonstrating how optimism in posterior sampling improves sample efficiency. The study provides bounds on learning performance under various conditions, offering insights into practical algorithm design.                                                                                                                                                                                                                                                                                                                                                                                                                        |
|                      |                                                                                                                              |                                                                                                                    |                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| More General         | User-guided reinforcement learning of robot assistive tasks for an intelligent environment                                   | <https://ieeexplore.ieee.org/document/1250666>                                                                     | Easy RL for SmartHome applications based on user inputs                                             | Jiming Liu et. al. researches training robots via combined user guidance and reinforcement learning, enabling non-experts to efficiently teach assistive tasks (e.g., retrieving medicine) in smart homes by integrating multi-level commands.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| More General         | Learning Concepts from Sensor Data of A Mobile Robot                                                                         | <https://link.springer.com/content/pdf/10.1023/A%3A1018245209731.pdf>                                              | Things to learn from robots learning behaviour and route data                                       | Machine learning enhances robot flexibility and efficiency. Existing methods target either high-level planning or low-level control. This paper bridges both levels by learning a hierarchy of concepts from a mobile robot’s route data. These concepts integrate perception and action at every level, enabling clearer human-robot communication. The GRDT relational learning algorithm is introduced, which exhaustively searches a hypothesis space constrained by user-defined grammar rules (rule schemata). This structured approach ensures concepts are perceptually grounded while aligning with human-defined frameworks.                                                                                                                                                                                                                                       |
| More Specific        | Machine Vision and Machine Learning for Intelligent Agrobots: A Review                                                       | <https://ieeexplore.ieee.org/document/9075599>                                                                     | Adaptability in agrobots                                                                            | Autonomous agricultural robots (Agrobots) perform tasks like harvesting, weed/disease detection, and fertilizing in unstructured environments. Existing systems use machine vision and AI to navigate (via UGVs/UAVs) and execute tasks, reducing labor and boosting food quality. This paper focuses on Agrobots’ adaptability in diverse settings, integrating machine learning for perception (detection, analysis) and control, alongside mapping via vision systems. Multi-robot and human-robot collaboration minimize waste, enhance sustainability, and ensure climate resilience. The study highlights how trained datasets and autonomous design enable flexible, efficient farming practices, advancing economic and environmental outcomes.                                                                                                                      |
| More General         | Human-robot Collaboration and Machine Learning: A Systematic Review of Recent Research                                       | <https://arxiv.org/abs/2110.07448>                                                                                 | Various methods comparison in robot ML                                                              | Human-robot collaboration (HRC) combines human and robot capabilities (physical/cognitive) to achieve shared goals. Machine learning (ML) is key for building adaptive cognitive models that process environmental and user inputs. This paper reviews 45 studies applying ML to HRC, clustering works by collaborative tasks (e.g., assembly, navigation), evaluation metrics, and cognitive variables (e.g., intent recognition). It analyzes ML algorithms (e.g., neural networks, reinforcement learning) and sensing modalities (vision, force sensors), emphasizing the need for time-dependent models (e.g., handling dynamic interactions). Cross-analysis reveals trends, such as growing use of multimodal sensing, and offers guidelines for future HRC research, addressing gaps like limited real-world validation and understudied social interaction aspects. |
| More General         | Smart Waste Collecting Robot Integration With IoT and Machine Learning                                                       | <https://ieeexplore.ieee.org/document/10391813>                                                                    | Improving efficiency for Waste-collector robots                                                     | Autonomous waste-collection robots use IoT to integrate components (Raspberry Pi central unit, camera, ultrasonic sensors, robotic arm) for navigation, obstacle avoidance, and waste detection. A CNN-based ML model trained on waste images enables real-time identification and categorization. The system combines sensor data (vision, distance) with precise arm control to operate independently, reducing manual labor and improving waste management efficiency.                                                                                                                                                                                                                                                                                                                                                                                                    |
|                      |                                                                                                                              |                                                                                                                    |                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| More Specific        | Effective Integration of Imitation Learning and Reinforcement Learning By Generating Internal Reward                         | <https://ieeexplore.ieee.org/document/4696448>                                                                     | imitation learning (IL) & reinforcement learning (RL), inspired by human learning                   | This paper combines imitation learning (IL) and reinforcement learning (RL) into a unified framework, inspired by human learning (observation + trial/error). An internal reward system, generated by the agent, bridges IL (reduces training time) and RL (optimizes long-term goals), minimizing reliance on excessive trial-and-error. Experiments validate the architecture’s efficiency, showing faster convergence and improved task performance compared to standalone RL or IL approaches.                                                                                                                                                                                                                                                                                                                                                                           |
| More Specific        | Using Machine Teaching to Investigate Human Assumptions When Teaching Reinforcement Learners                                 | <https://arxiv.org/abs/2009.02476>                                                                                 | Use Q-learning to balance exploration-exploitation                                                  | This study explores how humans teach AI agents via rewards/punishments, assuming learners use Q-learning (balancing exploration-exploitation). Researchers compare human teaching strategies to a machine teaching benchmark, optimized via deep learning to predict feedback’s impact on learner states. Experiments show humans teach effectively when Q-learners use low discount rates (prioritizing short-term rewards) and high learning rates (quickly updating beliefs), but their feedback remains suboptimal versus the model. Real-time visibility into learner states slightly improves teaching efficiency. Findings highlight mismatches between human intuition and machine learning dynamics, guiding AI designs that align with natural human teaching behaviors.                                                                                           |
| More Specific        | REIN-2: Giving Birth to Prepared Reinforcement Learning Agents Using Reinforcement Learning Agents                           | <https://arxiv.org/abs/2110.05128>                                                                                 | Use REIN-2, where a meta-learner (meta-RL agent) trains inner-learners (standard RL agents)         | Deep RL struggles with sample inefficiency, limiting industrial use. This paper introduces REIN-2, a meta-learning framework where a meta-learner (meta-RL agent) trains inner-learners (standard RL agents) to solve tasks. By reframing RL-environment interactions as a meta-environment, REIN-2 shifts focus from task-solving to learning how to learn, boosting stability and sample efficiency. Tests in OpenAI Gym (e.g., Mountain Car) show REIN-2 outperforms state-of-the-art Deep RL in scoring and training speed, proving its potential to bridge the gap between academic benchmarks and real-world applications.                                                                                                                                                                                                                                             |
| More Specific        | Personalized Learning Path Generation in E-Learning Systems Using Reinforcement Learning and Generative Adversarial Networks | <https://ieeexplore.ieee.org/document/9658967>                                                                     | Use Felder-Silverman models while minimizing direct learner feedback for E-learning personalization | E-learning personalization, accelerated by pandemic-driven online education, aims to tailor learning paths/content to individual styles (using Felder-Silverman models) while minimizing direct learner feedback. This paper combines reinforcement learning (RL) to optimize paths/objects and conditional GANs to simulate learner performance. The GAN rapidly adapts to student traits, generating synthetic data to train the RL strategy, reducing reliance on real-time interactions. Initial tests with synthetic data show the hybrid model efficiently personalizes paths across learner types and outperforms non-GAN systems, balancing customization with minimal user friction.                                                                                                                                                                                |
| More General         | Cooperative Multi-Agent Learning: The State of the Art                                                                       | <https://link.springer.com/content/pdf/10.1007/s10458-005-2631-2.pdf>                                              | Use various cooperative techniques such as centralized and decentralized optimization               | Cooperative multi-agent systems (MAS) face challenges in scalability and complexity as agents collaborate to solve tasks. This survey analyzes machine learning approaches (RL, evolutionary computation, game theory) for automating MAS design, categorizing methods into team learning (centralized optimization of joint actions) and concurrent learning (decentralized, agent-specific learners). It addresses communication strategies (direct/indirect), task decomposition, and scalability, while highlighting gaps in adaptive dynamics. Unlike prior surveys focused on subfields (e.g., RL), this cross-disciplinary review synthesizes insights across robotics, agent modeling, and complex systems. Concludes with MAS problem domains and resources, providing a roadmap for advancing adaptive, large-scale cooperative systems.                           |
| More General         | Intelligent Robot Path Planning and Navigation based on Reinforcement Learning and Adaptive Control                          | <https://www.aasmr.org/liss/Vol.10/No.3%202023/Vol.10.No.3.18.pdf>                                                 | Use Adaptive Control                                                                                | This study explores how reinforcement learning and adaptive control enhance intelligent robot path planning and navigation. By reviewing relevant literature and developing a new method, experimental results confirm significant performance improvements. The findings contribute to advancing robot navigation in complex environments, providing theoretical and methodological support for efficient task execution.                                                                                                                                                                                                                                                                                                                                                                                                                                                   |

Many of these research looked to improve computational efficiency and AI decision-making for robots.
Broader studies explore feedback-driven RL, process supervision, and model-based RL, addressing challenges like learning efficiency, human-AI interaction, and adaptive multi-agent collaboration.
These findings present RL’s potential in optimizing decision-making, reducing computational overhead, and improving real-world AI applications, while also revealing gaps in scalability, human-guided reinforcement, and generalization across tasks.
Future research should focus on enhancing sample efficiency, human-AI alignment, and the integration of RL with emerging technologies like generative models and IoT systems.
